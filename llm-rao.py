import re
from typing import Dict, List, Tuple, Any
import numpy as np
from openai import OpenAI

from config import Config
from env import Env

# ğŸ”¹ í‰ê°€ í•¨ìˆ˜: MIMOìš© evaluate.pyë¥¼ no-MIMOì— ë§ê²Œ ì–´ëŒ‘í„°ë¡œ ì‚¬ìš©
from evaluate import evaluate_mapping, summarize_feedback

import os
from dotenv import load_dotenv

load_dotenv()


# ============================ ì„¤ì • ============================

# ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì´ë¦„
OPENAI_MODEL = "gpt-4.1-mini"  # í•„ìš”í•˜ë©´ "gpt-4.1", "gpt-4.1-mini" ë“±ìœ¼ë¡œ ë³€ê²½

# LLM ì¶œë ¥ íŒŒì‹±ìš© ì •ê·œì‹: "RB g -> user k" ë˜ëŠ” "RB g -> user -1"
_PAT_LINE = re.compile(r"RB\s*(?P<rb>\d+)\s*->\s*user\s*(?P<user>-?\d+)", re.IGNORECASE)


# ===================== RB ì¸ë±ìŠ¤ í—¬í¼ =====================


def build_global_rb_index(cfg: Config) -> List[Tuple[int, int]]:
    """
    global RB index -> (layer, rb_idx) ë§¤í•‘ ìƒì„±.

    ì˜ˆ: layer_rb_counts = [37, 16, 8] ì´ë©´
        global_rb 0..36  -> (0, 0..36)
        global_rb 37..52 -> (1, 0..15)
        global_rb 53..60 -> (2, 0..7)
    """
    mapping: List[Tuple[int, int]] = []
    for l, cnt in enumerate(cfg.layer_rb_counts):
        for i in range(cnt):
            mapping.append((l, i))
    return mapping


# ===================== í”„ë¡¬í”„íŠ¸ ìƒì„± =====================


def summarize_state_for_prompt(state: Dict[str, np.ndarray], cfg: Config) -> str:
    """
    í˜„ì¬ ìŠ¬ë¡¯ ìƒíƒœ(state)ë¥¼ LLMì—ê²Œ ë³´ì—¬ì¤„ ìš”ì•½ ë¬¸ìì—´ë¡œ ë³€í™˜.
    - ì‹œê°„ t
    - ìœ ì €ë³„ ê±°ë¦¬ / í / SNR(dB)
    - ë ˆì´ì–´ë³„ RB ê°œìˆ˜
    """
    t = int(state["t"])
    distances = state["distances"]
    queues = state["queues"]
    snr = state["snr"]

    snr_db = 10 * np.log10(snr + 1e-12)

    lines = []
    lines.append(f"[Time slot t = {t}]")
    lines.append(
        f'- Objective: {cfg.objective}  ("rate"=sum-rate, "pf"=proportional fairness)'
    )
    lines.append(f"- Num users K = {cfg.num_users}")
    lines.append(f"- Num layers L = {cfg.num_layers}")
    lines.append(f"- Layer RB counts = {cfg.layer_rb_counts}")
    lines.append("")
    lines.append("Per-user status (index, distance[m], queue[bits], SNR[dB]):")

    for k in range(cfg.num_users):
        lines.append(
            f"  User {k}: d = {distances[k]:.1f} m, "
            f"Q = {queues[k]:.1f} bits, "
            f"SNR = {snr_db[k]:.1f} dB"
        )

    return "\n".join(lines)


def target_format_example(cfg: Config) -> str:
    """
    LLMì—ê²Œ ìš”êµ¬í•  ì¶œë ¥ í˜•ì‹ì„ ê°•í•˜ê²Œ ì œí•œí•œ ë¬¸ìì—´.
    ì„¤ëª…/í•´ì„¤ ê¸ˆì§€, ì •í™•íˆ R_totì¤„ë§Œ ì¶œë ¥í•˜ë„ë¡ ê°•ì¡°.
    ì¤‘ë³µ ìœ ì €/ê²¹ì¹˜ëŠ” RBë¥¼ ì“°ë©´ í•´ë‹¹ ì¤„ì€ ë²„ë ¤ì ¸ ì ìˆ˜ 0 ì²˜ë¦¬ëœë‹¤ê³  ê²½ê³ .
    """
    total_rbs = sum(cfg.layer_rb_counts)
    return f"""IMPORTANT: Output EXACTLY {total_rbs} lines and NOTHING ELSE.
Do NOT include explanations, notes, or extra text.

Each line must follow this format (global RB index g, user index k):
RB g -> user k

Allowed values:
- g: integer in [0..{total_rbs-1}]
- k: -1 (unassigned) or integer in [0..{cfg.num_users-1}]
- Each user may appear at MOST ONCE across all lines (c1). If you repeat a user, that line is discarded and score becomes 0.
- Do NOT assign two RBs that overlap the same base RB. If you do, the latter line is discarded and score becomes 0.

Example (short form; still you must output all {total_rbs} lines):
RB 0 -> user 3
RB 1 -> user -1
RB 2 -> user 1
...
RB {total_rbs-1} -> user -1
"""


def build_history_text(candidates: List[Dict[str, Any]]) -> str:
    """
    OPROìš©: ì´ë²ˆ ìŠ¬ë¡¯ì—ì„œ ì´ì „ ì‹œë„ë“¤ì˜ í‰ê°€ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ìš”ì•½.

    ê° candidateì—ëŠ” "eval_feedback" ë¬¸ìì—´ì´ ë“¤ì–´ ìˆë‹¤ê³  ê°€ì •.
    """
    if not candidates:
        return ""

    lines = []
    lines.append("[Previous attempts and feedback in this time slot]")
    for i, cand in enumerate(candidates):
        lines.append(f"Attempt {i}:")
        # summarize_feedback ê²°ê³¼ ë¬¸ìì—´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        fb = cand.get("eval_feedback", "").strip()
        if fb:
            lines.append(fb)
        # ì¤‘ë³µ/ê²¹ì¹¨ì´ ìˆì—ˆë‹¤ë©´ ëª…ì‹œì ìœ¼ë¡œ ê²½ê³  ì¶”ê°€
        if "c1 violated" in fb or "c2 violated" in fb:
            lines.append(
                "Warning: Duplicate user or overlapping RB detected. "
                "Do NOT repeat users and avoid overlapping base RBs. "
                "Lines that violate c1/c2 are discarded â†’ score drops."
            )
        # sanitize ë‹¨ê³„ì—ì„œ ë²„ë¦° ì¤„ ìˆ˜ë¥¼ ì•Œë ¤ì¤Œ
        drop_c1 = cand.get("drop_c1", 0)
        drop_c2 = cand.get("drop_c2", 0)
        if drop_c1 or drop_c2:
            lines.append(
                f"Note: {drop_c1} lines dropped due to duplicate users (c1), "
                f"{drop_c2} lines dropped due to overlapping RBs (c2)."
            )
        lines.append("")  # ë¹ˆ ì¤„

    return "\n".join(lines)


def build_prompt_for_llm(
    state: Dict[str, np.ndarray],
    cfg: Config,
    history_text: str = "",
) -> str:
    """
    LLMì—ê²Œ ë„˜ê¸¸ ìµœì¢… í”„ë¡¬í”„íŠ¸ = ìƒíƒœ ìš”ì•½ + ì¶œë ¥ í˜•ì‹ ì„¤ëª… + (ì„ íƒ) ì´ì „ ì‹œë„ í”¼ë“œë°±.
    """
    header = summarize_state_for_prompt(state, cfg)
    fmt = target_format_example(cfg)

    prompt = header + "\n\n" + fmt

    if history_text:
        # ì´ì „ ì‹œë„ë“¤ì˜ ì ìˆ˜/ì œì•½ ìœ„ë°˜ ì •ë³´ë¥¼ ê°™ì´ ë³´ì—¬ì£¼ê³ ,
        # ê·¸ê±¸ ì°¸ê³ í•´ì„œ ë” ë‚˜ì€ í•´ë¥¼ ë‚´ë¼ê³  ìš”ì²­
        prompt += (
            "\n\n"
            + history_text
            + "\n\nUsing the feedback from the previous attempts above, "
            "propose a NEW and IMPROVED allocation for THIS time slot. "
            "Do NOT repeat the same mistakes (constraints c1/c2 violations)."
        )

    return prompt


# ===================== LLM ì‘ë‹µ íŒŒì‹± =====================


def parse_allocation_from_llm_output(
    text: str,
    cfg: Config,
) -> List[np.ndarray]:
    """
    LLM ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ Env.step ì—ì„œ ìš”êµ¬í•˜ëŠ” allocation í˜•ì‹
    (layerë³„ np.ndarray) ìœ¼ë¡œ ë³€í™˜í•œë‹¤.

    1) ë¨¼ì € global RB index g (0..sum-1) ë¥¼ layer, rb_idxë¡œ ë§¤í•‘
    2) ê° ë¼ì¸: "RB g -> user k"
       - k == -1 ì´ë©´ ë¯¸í• ë‹¹
       - 0 <= k < num_users ì´ë©´ í•´ë‹¹ ìœ ì € í• ë‹¹
    3) ê²°ê³¼: allocation[l][i] = user index ë˜ëŠ” -1
    """
    total_rbs = sum(cfg.layer_rb_counts)
    global2li = build_global_rb_index(cfg)

    # ì´ˆê¸°ê°’: ì „ë¶€ ë¯¸í• ë‹¹(-1)
    allocation: List[np.ndarray] = [
        -1 * np.ones(cfg.layer_rb_counts[l], dtype=int) for l in range(cfg.num_layers)
    ]

    mapping: Dict[int, int] = {}  # g -> user
    for rb_str, user_str in _PAT_LINE.findall(text):
        g = int(rb_str)
        k = int(user_str)
        mapping[g] = k

    # ë§¤í•‘ëœ gì— ëŒ€í•´ allocation ì±„ìš°ê¸°
    for g, k in mapping.items():
        if not (0 <= g < total_rbs):
            # ë²”ìœ„ ë°–ì´ë©´ ë¬´ì‹œ (ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬ ê°€ëŠ¥)
            continue
        l, i = global2li[g]
        allocation[l][i] = k

    return allocation


def sanitize_allocation(allocation: List[np.ndarray], cfg: Config) -> Tuple[List[np.ndarray], int, int]:
    """
    ê°„ë‹¨ ë³´ì •: í•œ ìœ ì €ëŠ” í•œ ë²ˆë§Œ, base RB ê²¹ì¹˜ë©´ í›„ìˆœìœ„ RBë¥¼ -1ë¡œ ë¹„ìš´ë‹¤.
    (LLMì´ c1/c2ë¥¼ ì–´ê²¼ì„ ë•Œ ìë™ìœ¼ë¡œ ì •ë¦¬)
    """
    cleaned = [arr.copy() for arr in allocation]
    used_users = set()
    used_base = np.zeros(cfg.layer0_rb, dtype=bool)
    global2li = build_global_rb_index(cfg)
    drop_c1 = 0
    drop_c2 = 0

    for g, (l, i) in enumerate(global2li):
        k = int(cleaned[l][i])
        if k < 0:
            continue
        # c1: ì´ë¯¸ ë°°ì •ëœ ìœ ì €ë©´ ë¹„ì›€
        if k in used_users:
            cleaned[l][i] = -1
            drop_c1 += 1
            continue
        # c2: base RB ê²¹ì¹˜ë©´ ë¹„ì›€
        base_indices = cfg.layer_to_base_rb[l][i]
        if any(used_base[b] for b in base_indices):
            cleaned[l][i] = -1
            drop_c2 += 1
            continue
        used_users.add(k)
        for b in base_indices:
            used_base[b] = True

    return cleaned, drop_c1, drop_c2


def allocation_to_mapping(
    allocation: List[np.ndarray],
    cfg: Config,
) -> Dict[int, List[int]]:
    """
    Env.step()ì— ì£¼ëŠ” allocation (layerë³„ ë°°ì—´)ì„
    evaluate_mapping()ì´ ê¸°ëŒ€í•˜ëŠ” {RB: [users...]} í˜•ì‹ìœ¼ë¡œ ë³€í™˜.

    - global RB index g: 0..R_tot-1
    - no-MIMO ì´ë¯€ë¡œ, ê° RBì—ëŠ” 0ëª…([]) ë˜ëŠ” 1ëª…([k])ë§Œ í—ˆìš©.
    """
    global2li = build_global_rb_index(cfg)
    mapping: Dict[int, List[int]] = {}

    for g, (l, i) in enumerate(global2li):
        k = int(allocation[l][i])
        if k < 0:
            mapping[g] = []
        else:
            mapping[g] = [k]

    return mapping


def build_adapter_env_for_evaluator(env_obj: Env, cfg: Config) -> Dict[str, Any]:
    """
    í˜„ì¬ Env ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,
    evaluate_mapping()ì´ ê¸°ëŒ€í•˜ëŠ” env ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜.

    í•„ìš”í•œ í•„ë“œ:
      - "config": num_users, num_rbs, N_ant_ap, T
      - "zf_rates_for_set": (rb, users[]) -> per-user rate ë²¡í„°
      - "rb_overlap_mask": (R,R) bool ë°°ì—´
      - "G_of_rb": RBë³„ ë™ì‹œ ì‚¬ìš©ì ìƒí•œ (no-MIMO â†’ ì „ë¶€ 1)
      - "q_backlog_bits": ê¸¸ì´ K ë°°ì—´
      - "R_min_bps": (ì„ íƒ, ì—¬ê¸°ì„  None)
    """

    class EvalConfig:
        pass

    K = env_obj.num_users
    total_rbs = sum(cfg.layer_rb_counts)
    N_ant_ap = getattr(cfg, "num_antennas", 1)

    eval_cfg = EvalConfig()
    eval_cfg.num_users = K
    eval_cfg.num_rbs = total_rbs
    eval_cfg.N_ant_ap = N_ant_ap
    eval_cfg.T = cfg.T

    # --- global RB -> (layer, i) & base RB ì§‘í•© ìƒì„± ---
    global2li: List[Tuple[int, int]] = []
    base_sets: List[set] = []
    for l, cnt in enumerate(cfg.layer_rb_counts):
        for i in range(cnt):
            global2li.append((l, i))
            base_sets.append(set(cfg.layer_to_base_rb[l][i]))

    R = total_rbs
    overlap = np.zeros((R, R), dtype=bool)
    for a in range(R):
        for b in range(a + 1, R):
            if base_sets[a] & base_sets[b]:
                overlap[a, b] = overlap[b, a] = True

    # no-MIMO: RBë‹¹ ìµœëŒ€ 1ëª…
    G_of_rb = np.ones(R, dtype=int)

    # í: bits ë‹¨ìœ„ë¼ê³  ê°€ì • (Envì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
    q_bits = env_obj.queues.copy()
    snr_linear = env_obj.snr_linear.copy()
    rb_bw = cfg.rb_bandwidth

    def zf_rates_for_set(rb: int, users: List[int]) -> np.ndarray:
        """
        no-MIMO í™˜ê²½ì„ evaluate.pyì˜ zf_rates_for_set ì¸í„°í˜ì´ìŠ¤ì— ë§ê²Œ ë˜í•‘.
        - len(users)==0ì´ë©´ ë¹ˆ ë²¡í„° ë°˜í™˜
        - len(users)>=1 ì´ë©´, ê° ìœ ì €ì— ëŒ€í•´ Envì™€ ë™ì¼ ê³µì‹ì˜ rate ê³„ì‚°
        - MU-MIMOëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì—¬ëŸ¬ userë¥¼ ì£¼ë”ë¼ë„
          ë‹¨ìˆœíˆ 'ê°ê° ë‹¨ì¼ ìœ ì €ì¼ ë•Œ rate'ë¡œ ê³„ì‚° (ì–´ì°¨í”¼ c3ë¡œ ê±¸ëŸ¬ì§).
        """
        if len(users) == 0:
            return np.zeros(0, dtype=float)

        l, i = global2li[rb]
        base_rbs = cfg.layer_to_base_rb[l][i]
        B_li = len(base_rbs) * rb_bw

        rates = []
        for u in users:
            gamma_k = snr_linear[u]
            r_li_k = B_li * np.log2(1.0 + gamma_k)
            rates.append(r_li_k)
        return np.array(rates, dtype=float)

    adapter_env: Dict[str, Any] = {
        "config": eval_cfg,
        "zf_rates_for_set": zf_rates_for_set,
        "rb_overlap_mask": overlap,
        "G_of_rb": G_of_rb,
        "q_backlog_bits": q_bits,
        "R_min_bps": None,
    }
    return adapter_env


# ===================== OpenAI LLM í˜¸ì¶œ =====================


def call_openai_llm(prompt: str, model: str = OPENAI_MODEL) -> str:
    """
    OpenAI Responses APIë¥¼ ì‚¬ìš©í•´ LLMì„ í˜¸ì¶œí•˜ê³ ,
    plain text í˜•íƒœì˜ ì¶œë ¥ì„ ë°˜í™˜.

    í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
    """
    client = OpenAI()

    response = client.responses.create(
        model=model,
        input=prompt,
        max_output_tokens=1500,  # 61ì¤„ ê°•ì œ ì¶œë ¥ ëŒ€ë¹„ í† í° ì—¬ìœ  í™•ë³´
    )

    texts: List[str] = []
    for item in response.output[0].content:
        if getattr(item, "type", None) == "output_text":
            txt = getattr(getattr(item, "output_text", None), "text", None)
            if txt:
                texts.append(txt)

    if not texts:
        return str(response)

    return "".join(texts)


# ===================== ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„ (OPRO ë²„ì „) =====================


def run_llm_ra_episode(
    cfg: Config,
    num_slots: int,
    model: str = OPENAI_MODEL,
    inner_iters: int = 3,  # OPRO: ìŠ¬ë¡¯ë‹¹ LLM ì‹œë„ íšŸìˆ˜
):
    """
    LLM ê¸°ë°˜ RA ì—í”¼ì†Œë“œ ì‹¤í–‰ (no-MIMO + Env + evaluate.py + OPRO):

    ê° ìŠ¬ë¡¯ë§ˆë‹¤:
      1) í˜„ì¬ stateì™€ Envë¥¼ ê¸°ë°˜ìœ¼ë¡œ adapter_env ìƒì„± (í‰ê°€ìš©)
      2) inner_itersë²ˆ ë°˜ë³µ:
         - ì´ì „ ì‹œë„ë“¤ì˜ eval_feedbackì„ history_textë¡œ ë§Œë“¤ì–´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
         - LLM í˜¸ì¶œ â†’ allocation íŒŒì‹±
         - allocationì„ mappingìœ¼ë¡œ ë³€í™˜ â†’ evaluate_mapping(...) í˜¸ì¶œ
         - eval_feedback(summarize_feedback) ì €ì¥
      3) inner_itersê°œ candidate ì¤‘ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ allocationì„ ì„ íƒ
      4) ê·¸ allocationì„ Env.step()ì— ë„£ì–´ ì‹¤ì œ í/ì±„ë„ ì—…ë°ì´íŠ¸ + reward ê³„ì‚°
      5) ë¡œê·¸ ê¸°ë¡
    """
    env = Env(cfg)
    state = env.reset()

    history = []
    total_reward = 0.0

    for t in range(num_slots):
        print(f"\n================ Slot {t} ================")

        # í˜„ì¬ Env ìƒíƒœë¥¼ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€ í™˜ê²½(adapter_env) êµ¬ì„±
        adapter_env = build_adapter_env_for_evaluator(env, cfg)

        # OPROìš©: ì´ë²ˆ ìŠ¬ë¡¯ì—ì„œì˜ candidateë“¤
        candidates: List[Dict[str, Any]] = []

        for inner in range(inner_iters):
            # ì´ì „ ì‹œë„ë“¤ì˜ feedbackì„ history_textë¡œ ë§Œë“¦
            history_text = build_history_text(candidates)

            print(f"\n  [Inner iter {inner}] Calling LLM...")
            prompt = build_prompt_for_llm(state, cfg, history_text=history_text)
            llm_output = call_openai_llm(prompt, model=model)

            # LLM ì¶œë ¥ì—ì„œ allocation íŒŒì‹±
            allocation_raw = parse_allocation_from_llm_output(llm_output, cfg)
            allocation, drop_c1, drop_c2 = sanitize_allocation(allocation_raw, cfg)

            # evaluate.pyìš© mapping ìƒì„±
            mapping = allocation_to_mapping(allocation, cfg)

            # í‰ê°€ (ì œì•½/ì ìˆ˜) - ì œì•½ ìœ„ë°˜ ì‹œ score=0ìœ¼ë¡œ ì²˜ë¦¬
            eval_result = evaluate_mapping(
                mapping,
                adapter_env,
                objective=cfg.objective,
                zero_on_violation=True,
            )
            eval_feedback = summarize_feedback(eval_result)

            print("    [Eval] " + eval_feedback.replace("\n", "\n    [Eval] "))

            candidates.append(
                {
                    "allocation": allocation,
                    "mapping": mapping,
                    "eval_result": eval_result,
                    "eval_feedback": eval_feedback,
                    "llm_output": llm_output,
                    "drop_c1": drop_c1,
                    "drop_c2": drop_c2,
                }
            )

        # === inner_itersê°œ ì¤‘ ê°€ì¥ ë†’ì€ scoreë¥¼ ê°€ì§„ candidate ì„ íƒ ===
        best_idx = 0
        best_score = candidates[0]["eval_result"]["score"]
        for i in range(1, len(candidates)):
            sc = candidates[i]["eval_result"]["score"]
            if sc > best_score:
                best_score = sc
                best_idx = i

        best_cand = candidates[best_idx]
        best_allocation = best_cand["allocation"]
        best_eval = best_cand["eval_result"]

        print(
            f"\n  [Selection] Chosen attempt = {best_idx}, "
            f"score = {best_eval['score']:.4f}"
        )

        # === ì„ íƒí•œ allocationì„ Env.step()ì— ë„£ì–´ ì‹¤ì œ reward ê³„ì‚° ===
        try:
            next_state, reward = env.step(best_allocation)
            violated_env = False
            violation_msg_env = ""
        except AssertionError as e:
            violated_env = True
            violation_msg_env = str(e)
            print(f"  [Env] Constraint violated in Env.step: {violation_msg_env}")
            next_state, reward = state, 0.0

        total_reward += reward

        print(f"  [Env] Reward (objective={cfg.objective}) = {reward:.4f}")

        # ë¡œê·¸ ê¸°ë¡
        history.append(
            {
                "t": t,
                "state": state,
                "chosen_allocation": best_allocation,
                "chosen_eval_result": best_eval,
                "reward": reward,
                "violated_env": violated_env,
                "violation_msg_env": violation_msg_env,
                "candidates": candidates,
            }
        )

        state = next_state

    avg_reward = total_reward / float(num_slots) if num_slots > 0 else 0.0

    return {
        "total_reward": total_reward,
        "avg_reward": avg_reward,
        "history": history,
    }


# ===================== ì‹¤í–‰ ì˜ˆì‹œ (Scenario 1 & 4) =====================


def run_scenario(objective: str, num_slots: int):
    """
    objective = "rate" (Scenario 1) ë˜ëŠ” "pf" (Scenario 4)
    ì— ëŒ€í•´ LLM-RA ì—í”¼ì†Œë“œë¥¼ í•œ ë²ˆ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥.
    """
    cfg = Config()
    cfg.objective = objective

    print("=" * 60)
    print(f"Running LLM-RA episode: objective = {objective}, num_slots = {num_slots}")
    print("=" * 60)

    result = run_llm_ra_episode(cfg, num_slots=num_slots, model=OPENAI_MODEL)

    print("\n=== Episode summary ===")
    print(f"Objective            : {objective}")
    print(f"Total reward (Env)   : {result['total_reward']:.4f}")
    print(f"Average per slot     : {result['avg_reward']:.4f}")
    num_viol_env = sum(1 for h in result["history"] if h["violated_env"])
    print(f"Slots with Env violation: {num_viol_env}/{num_slots}")

    return result


def main():
    # ì˜ˆì‹œ: ê° ì‹œë‚˜ë¦¬ì˜¤ 5ìŠ¬ë¡¯ì”©ë§Œ í…ŒìŠ¤íŠ¸ (ë‚˜ì¤‘ì— 50, 100ìœ¼ë¡œ ëŠ˜ë ¤ë„ ë¨)
    num_slots = 5

    # Scenario 1: sum-rate
    run_scenario("rate", num_slots)

    # Scenario 4: proportional fairness
    run_scenario("pf", num_slots)


if __name__ == "__main__":
    main()
